import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import matplotlib.font_manager as fm
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau
import sys
import os
from typing import List, Tuple

# ==========================================
# å­—ä½“è®¾ç½® (ä¿ç•™ä½ çš„ç¨³å®šé…ç½®)
# ==========================================
FONT_PROP = None
FONT_NAME = 'Arial Unicode MS' 
try:
    # å¼ºåˆ¶ä½¿ç”¨ TkAgg åç«¯
    import matplotlib
    matplotlib.use('TkAgg') 
    
    # å°è¯•æ³¨å†Œå­—ä½“
    CHINESE_FONT_PATH = "/System/Library/Fonts/Supplemental/Arial Unicode.ttf"
    if os.path.exists(CHINESE_FONT_PATH):
        FONT_PROP = fm.FontProperties(fname=CHINESE_FONT_PATH, size=12)
        plt.rcParams['font.sans-serif'] = [FONT_PROP.get_name()]
        plt.rcParams['axes.unicode_minus'] = False
        print(f"âœ… å­—ä½“æˆåŠŸè®¾ç½®: {FONT_PROP.get_name()}")

except Exception as e:
    print(f"âŒ å­—ä½“é…ç½®å¤±è´¥ï¼Œå›¾è¡¨å¯èƒ½ä¹±ç ã€‚é”™è¯¯: {e}")
    FONT_PROP = None

# æ¸…ç† TF å†…å­˜çŠ¶æ€å¹¶è®¾ç½®ç»˜å›¾é£æ ¼
tf.keras.backend.clear_session()
plt.style.use('seaborn-v0_8') 

# ==========================================
# 1. æ•°æ®å‡†å¤‡ä¸ç‰¹å¾å·¥ç¨‹
# ==========================================
def get_and_prepare_data(ticker: str = '000001.SS') -> pd.DataFrame:
    """è·å–æ•°æ®å¹¶æ·»åŠ  MA10/MA20 ç‰¹å¾"""
    print(f"æ­£åœ¨ä¸‹è½½ {ticker} æ•°æ®...")
    try:
        # ä¸‹è½½æ•°æ®
        df = yf.download(ticker, start='2019-10-01', end=None, progress=False)
    except Exception as e:
        print(f"ä¸‹è½½å¤±è´¥: {e}")
        sys.exit(1)
    
    if df.empty:
        print("æœªè·å–åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–è‚¡ç¥¨ä»£ç ã€‚")
        sys.exit(1)

    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.get_level_values(0)
        
    df = df[['Open', 'High', 'Low', 'Close', 'Volume']]
    
    # ç‰¹å¾å·¥ç¨‹
    df['MA10'] = df['Close'].rolling(window=10).mean()
    df['MA20'] = df['Close'].rolling(window=20).mean()
    # [æ–°å¢æƒ…ç»ªç‰¹å¾ 1]: æˆäº¤é‡ç›¸å¯¹å‡å€¼çš„æ¯”ç‡ (V_Ratio)
    df['V_MA30'] = df['Volume'].rolling(window=30).mean()
    df['V_Ratio'] = df['Volume'] / df['V_MA30']
    
    # [æ–°å¢æƒ…ç»ªç‰¹å¾ 2]: å†å²æ³¢åŠ¨ç‡ (Historical Volatility, 20æ—¥)
    # è®¡ç®—æ—¥å¯¹æ•°æ”¶ç›Šç‡
    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))
    # 20æ—¥å†å²æ³¢åŠ¨ç‡ (å¹´åŒ–ï¼Œä¹˜ä»¥sqrt(252))
    df['HV_20'] = df['Log_Return'].rolling(window=20).std() * np.sqrt(252)
    df = df.dropna()
     # [ä¿®æ”¹ç‚¹ 2]: æœ€ç»ˆç‰¹å¾åˆ—è¡¨ (ç§»é™¤ V_MA30 å’Œ Log_Return)
    df = df[['Open', 'High', 'Low', 'Close', 'MA10', 'MA20', 'V_Ratio', 'HV_20']]
    print(f"âœ… æ•°æ®å¤„ç†å®Œæˆã€‚ç‰¹å¾æ•°é‡: {df.shape[1]}")
    return df

# ==========================================
# 2. æ•°æ®é›†åˆ‡åˆ†ä¸å½’ä¸€åŒ–
# ==========================================
def split_and_scale(df: pd.DataFrame, look_back: int) -> tuple[np.ndarray, np.ndarray, MinMaxScaler, pd.DataFrame]:
    """ä¸¥æ ¼æŒ‰æ—¶é—´åˆ‡åˆ†å¹¶å½’ä¸€åŒ–"""
    train_df = df.loc['2020-01-01':'2024-12-31']
    test_df_raw = df.loc['2025-01-01':]

    full_dataset = pd.concat((train_df, test_df_raw), axis=0)
    test_inputs = full_dataset[len(full_dataset) - len(test_df_raw) - look_back:].values
    
    # å½’ä¸€åŒ–
    scaler = MinMaxScaler(feature_range=(0, 1))
    train_scaled = scaler.fit_transform(train_df.values)
    test_inputs_scaled = scaler.transform(test_inputs)
    
    print(f"âœ… æ•°æ®åˆ‡åˆ†å®Œæˆã€‚è®­ç»ƒæ ·æœ¬: {len(train_df)}, æµ‹è¯•æ ·æœ¬: {len(test_df_raw)}")
    return train_scaled, test_inputs_scaled, scaler, test_df_raw

def create_xy(dataset: np.ndarray, look_back: int) -> tuple[np.ndarray, np.ndarray]:
    """æ„é€  LSTM 3D æ•°æ®æ ¼å¼"""
    X, Y = [], []
    for i in range(look_back, len(dataset)):
        X.append(dataset[i-look_back:i, :])
        Y.append([dataset[i, 1], dataset[i, 3]]) 
    return np.array(X), np.array(Y)

# ==========================================
# 3. åŠ¨æ€æ¨¡å‹æ„å»º (ä¿®æ”¹ç‚¹: æ”¯æŒä¸åŒå±‚æ•°)
# ==========================================
def build_generic_lstm_model(layer_units: List[int], input_shape: Tuple[int, int]) -> Model:
    """
    æ ¹æ®ä¼ å…¥çš„å•å…ƒåˆ—è¡¨åŠ¨æ€æ„å»º LSTM æ¨¡å‹ã€‚
    ä¾‹å¦‚ layer_units=[100, 50] æ„å»ºä¸¤å±‚ï¼Œ[128] æ„å»ºä¸€å±‚ã€‚
    """
    model = Sequential()
    
    for i, units in enumerate(layer_units):
        # é€»è¾‘ï¼šå¦‚æœæ˜¯æœ€åä¸€å±‚ LSTMï¼Œreturn_sequences å¿…é¡»ä¸º False
        # å¦‚æœåé¢è¿˜æœ‰ LSTM å±‚ï¼Œreturn_sequences å¿…é¡»ä¸º True
        is_last_lstm_layer = (i == len(layer_units) - 1)
        return_seq = not is_last_lstm_layer
        
        if i == 0:
            # ç¬¬ä¸€å±‚å¿…é¡»æŒ‡å®š input_shape
            model.add(LSTM(units=units, return_sequences=return_seq, input_shape=input_shape))
        else:
            # åç»­å±‚è‡ªåŠ¨æ¨æ–­
            model.add(LSTM(units=units, return_sequences=return_seq))
            
        model.add(Dropout(0.3))
    
    model.add(Dense(units=2)) # è¾“å‡º High, Close
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
    return model

# å­¦ä¹ ç‡è°ƒåº¦å™¨
lr_schedule = ReduceLROnPlateau(
    monitor='val_loss', 
    factor=0.5,      
    patience=10,     
    min_lr=0.00001,  
    verbose=0 # å®éªŒæ—¶é™é»˜
)

# ==========================================
# 4. é¢„æµ‹ä¸è¯„ä¼°è¾…åŠ©å‡½æ•°
# ==========================================
def inverse_transform_helper(preds: np.ndarray, scaler: MinMaxScaler) -> tuple[np.ndarray, np.ndarray]:
    dummy = np.zeros((len(preds), 8))
    dummy[:, 1] = preds[:, 0]
    dummy[:, 3] = preds[:, 1]
    res = scaler.inverse_transform(dummy)
    return res[:, 1], res[:, 3]

def evaluate_predictions(real: np.ndarray, pred: np.ndarray) -> tuple[float, float]:
    rmse = np.sqrt(mean_squared_error(real, pred))
    mae = mean_absolute_error(real, pred)
    return rmse, mae

# ==========================================
# 5. ä¸»ç¨‹åºé€»è¾‘ (ä¿®æ”¹ç‚¹: å®éªŒå¾ªç¯)
# ==========================================
def main():
    LOOK_BACK = 30
    EPOCHS = 50
    BATCH_SIZE = 32

    # --- å®šä¹‰å®éªŒé…ç½® ---
    # é”®æ˜¯å®éªŒåç§°ï¼Œå€¼æ˜¯ LSTM å±‚ç»“æ„çš„åˆ—è¡¨
    # ä¾‹å¦‚ [128, 64] ä»£è¡¨ç¬¬ä¸€å±‚128ä¸ªå•å…ƒï¼Œç¬¬äºŒå±‚64ä¸ªå•å…ƒ
    EXPERIMENTS = {
        "Exp1_Single_Layer": [64],                 # å•å±‚
        "Exp1_Single_Layer_Narrow": [32],          # å•å±‚ (çª„)
        "Exp1_Single_Layer_Wide": [128],           # å•å±‚ (æ›´å®½)
        "Exp2_Two_Layers":   [128, 64],            # åŒå±‚ (åŸºå‡†)
        "Exp2_Two_Layers_Narrow":   [128, 64],     # åŒå±‚ (çª„)
        "Exp2_Two_Layers_Wide":   [256, 128],      # åŒå±‚ (æ›´å®½)
        "Exp3_Three_Layers": [128, 64, 32],        # ä¸‰å±‚ (æ·±å±‚)
        "Exp3_Three_Layers_Narrow": [64, 32, 16],  # ä¸‰å±‚ (çª„)
        "Exp3_Three_Layers_Wide": [256, 128, 64],  # ä¸‰å±‚ (å®½)
    }

    # 1. æ•°æ®å‡†å¤‡
    df = get_and_prepare_data()
    train_scaled, test_inputs_scaled, scaler, test_df_target = split_and_scale(df, LOOK_BACK)
    X_train, y_train = create_xy(train_scaled, LOOK_BACK)
    X_test, y_test = create_xy(test_inputs_scaled, LOOK_BACK)

    real_close = test_df_target['Close'].values
    real_high = test_df_target['High'].values
    dates = test_df_target.index

    results_data = [] # ç”¨äºå­˜å‚¨ç»“æœ

    print(f"\n======== å¼€å§‹ä¸åŒéšè—å±‚æ•°é‡çš„å¯¹æ¯”å®éªŒ ========")

    # 2. å¾ªç¯å®éªŒ
    for exp_name, layers_config in EXPERIMENTS.items():
        print(f"\n>> æ­£åœ¨è®­ç»ƒæ¨¡å‹: {exp_name} (ç»“æ„: {layers_config})...")
        
        # æ¸…ç†å†…å­˜
        tf.keras.backend.clear_session()
        
        # æ„å»ºæ¨¡å‹
        model = build_generic_lstm_model(layers_config, (X_train.shape[1], X_train.shape[2]))
        
        # è®­ç»ƒ (verbose=0 ä¸åˆ·å±ï¼Œåªæ˜¾ç¤ºç»“æœ)
        history = model.fit(
            X_train, y_train, 
            epochs=EPOCHS, 
            batch_size=BATCH_SIZE, 
            verbose=0, 
            validation_split=0.1,
            callbacks=[lr_schedule]
        )
        
        # é¢„æµ‹
        preds = model.predict(X_test, verbose=0)
        p_high, p_close = inverse_transform_helper(preds, scaler)
        
        # è¯„ä¼°
        rmse, mae = evaluate_predictions(real_close, p_close)
        val_loss = history.history['val_loss'][-1]
        
        print(f"   [å®Œæˆ] MAE(æ”¶ç›˜ä»·): {mae:.4f} | RMSE: {rmse:.4f} | Val_Loss: {val_loss:.6f}")
        
        results_data.append({
            "Experiment": exp_name,
            "Structure": str(layers_config),
            "Layers_Count": len(layers_config),
            "MAE": mae,
            "RMSE": rmse,
            "Val_Loss": val_loss,
            "Pred_High": p_high,   # æš‚å­˜é¢„æµ‹ç»“æœä»¥ä¾¿ç”»å›¾
            "Pred_Close": p_close
        })

    # 3. ç»“æœæ€»ç»“
    results_df = pd.DataFrame(results_data).sort_values(by="MAE")
    print("\n" + "="*50)
    print("å®éªŒç»“æœæ±‡æ€» (æŒ‰ MAE è¯¯å·®ä»å°åˆ°å¤§æ’åº)")
    print("="*50)
    print(results_df[["Experiment", "Structure", "MAE", "RMSE", "Val_Loss"]].to_string(index=False))
    
    # è·å–æœ€ä½³æ¨¡å‹çš„æ•°æ®
    best_exp = results_df.iloc[0]
    best_name = best_exp["Experiment"]
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹æ˜¯: {best_name} (MAE: {best_exp['MAE']:.4f})")

    # 4. ç»˜å›¾ (åªç»˜åˆ¶æœ€ä½³æ¨¡å‹çš„æ•ˆæœ)
    print(f"æ­£åœ¨ç»˜åˆ¶æœ€ä½³æ¨¡å‹ ({best_name}) çš„å›¾è¡¨...")
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)

    # ä¸Šå›¾ï¼šæœ€é«˜ä»·
    ax1.set_title(f'ä¸Šè¯æŒ‡æ•° 2025å¹´ æœ€é«˜ä»·é¢„æµ‹ (æœ€ä½³æ¨¡å‹: {best_name})', fontsize=14, fontproperties=FONT_PROP)
    ax1.plot(dates, real_high, label='å®é™…æœ€é«˜ä»·', color='#d62728', linewidth=2)
    ax1.plot(dates, best_exp["Pred_High"], label='é¢„æµ‹æœ€é«˜ä»·', color='#1f77b4', linestyle='--', linewidth=1.5)
    ax1.legend(loc='upper left', prop=FONT_PROP)
    ax1.grid(True, alpha=0.3)
    ax1.set_ylabel('ä»·æ ¼', fontproperties=FONT_PROP)

    # ä¸‹å›¾ï¼šæ”¶ç›˜ä»·
    ax2.set_title(f'ä¸Šè¯æŒ‡æ•° 2025å¹´ æ”¶ç›˜ä»·é¢„æµ‹ (æœ€ä½³æ¨¡å‹: {best_name})', fontsize=14, fontproperties=FONT_PROP)
    ax2.plot(dates, real_close, label='å®é™…æ”¶ç›˜ä»·', color='#2ca02c', linewidth=2)
    ax2.plot(dates, best_exp["Pred_Close"], label='é¢„æµ‹æ”¶ç›˜ä»·', color='#ff7f0e', linestyle='--', linewidth=1.5)
    ax2.legend(loc='upper left', prop=FONT_PROP)
    ax2.grid(True, alpha=0.3)
    ax2.set_ylabel('ä»·æ ¼', fontproperties=FONT_PROP)
    ax2.set_xlabel('æ—¥æœŸ', fontproperties=FONT_PROP)

    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
    plt.gcf().autofmt_xdate()

    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    SAVE_NAME = f'LSTM_Compare_Best_{best_name}.png'
    plt.savefig(SAVE_NAME, dpi=300)
    print(f"âœ… å›¾è¡¨å·²ä¿å­˜è‡³: {SAVE_NAME}")
    
    # å¼¹çª—æ˜¾ç¤º (å¯é€‰)
    # plt.show()

if __name__ == "__main__":
    main()