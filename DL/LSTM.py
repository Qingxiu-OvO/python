import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib
# [å…³é”®è®¾ç½®]: å¼ºåˆ¶ä½¿ç”¨éäº¤äº’å¼åç«¯ Agg (ä¿å­˜å›¾ç‰‡ä¸“ç”¨ï¼Œé˜²æ­¢åœ¨æ— å¤´æœåŠ¡å™¨æˆ–Macä¸Šå¼¹çª—æŠ¥é”™)
matplotlib.use('Agg') 
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import matplotlib.font_manager as fm
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau
import sys
import os
from typing import List, Tuple

# ==============================================================================
# 0. åŸºç¡€è®¾ç½® (éšæœºç§å­ & å­—ä½“é…ç½®)
# ==============================================================================

# åŸºç¡€éšæœºç§å­ (åç»­ä¼šåœ¨æ¯è½®è®­ç»ƒä¸­å¾®è°ƒè¿™ä¸ªç§å­ä»¥ä¿è¯ç‹¬ç«‹æ€§)
BASE_SEED = 42
os.environ['PYTHONHASHSEED'] = str(BASE_SEED)

# --- å­—ä½“è®¾ç½® (Mac/Linux/Windows å…¼å®¹æ€§å¤„ç†) ---
# ç›®çš„ï¼šè§£å†³ Matplotlib æ— æ³•æ˜¾ç¤ºä¸­æ–‡ï¼Œæ˜¾ç¤ºä¸ºæ–¹æ¡†çš„é—®é¢˜
FONT_PROP = None
FONT_NAME = 'sans-serif'

# Mac ç³»ç»Ÿå¸¸ç”¨ .ttf ä¸­æ–‡å­—ä½“è·¯å¾„ (ä¼˜å…ˆçº§ä»é«˜åˆ°ä½)
# ä¼˜å…ˆä½¿ç”¨ .ttf æ–‡ä»¶ï¼Œé¿å… .ttc é›†åˆæ–‡ä»¶å¯¼è‡´çš„åº•å±‚è¯»å–é”™è¯¯
CANDIDATE_FONTS = [
    "/System/Library/Fonts/STHeiti Light.ttf",             # åæ–‡é»‘ä½“ (Macæœ€ç¨³)
    "/System/Library/Fonts/Supplemental/Arial Unicode.ttf", # é€šç”¨ Unicode
    "/Library/Fonts/Arial Unicode.ttf"
]

try:
    found_font = False
    # éå†å€™é€‰åˆ—è¡¨ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªå­˜åœ¨çš„å­—ä½“æ–‡ä»¶
    for path in CANDIDATE_FONTS:
        if os.path.exists(path):
            FONT_PROP = fm.FontProperties(fname=path, size=12)
            FONT_NAME = FONT_PROP.get_name()
            
            # è®¾ç½® Matplotlib å…¨å±€å‚æ•°
            plt.rcParams['font.family'] = 'sans-serif'
            plt.rcParams['font.sans-serif'] = [FONT_NAME]
            plt.rcParams['axes.unicode_minus'] = False # è§£å†³è´Ÿå·æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
            
            print(f"âœ… å­—ä½“é…ç½®æˆåŠŸ: {FONT_NAME} (è·¯å¾„: {path})")
            found_font = True
            break
    
    # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šè·¯å¾„ï¼Œå›é€€åˆ°ç³»ç»Ÿåç§°æŸ¥æ‰¾
    if not found_font:
        print("âš ï¸ æœªæ‰¾åˆ°é¢„è®¾è·¯å¾„å­—ä½“ï¼Œå°è¯•ç³»ç»Ÿè‡ªåŠ¨å›é€€...")
        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False

except Exception as e:
    print(f"âŒ å­—ä½“é…ç½®å¼‚å¸¸: {e}")

# æ¸…ç† TensorFlow ä¹‹å‰çš„ä¼šè¯ï¼Œé‡Šæ”¾æ˜¾å­˜/å†…å­˜
tf.keras.backend.clear_session()
# è®¾ç½®ç»˜å›¾é£æ ¼
plt.style.use('seaborn-v0_8') 

# ==============================================================================
# 1. æ•°æ®è·å–ä¸ç‰¹å¾å·¥ç¨‹
# ==============================================================================
def get_and_prepare_data(ticker: str = '000001.SS') -> pd.DataFrame:
    """
    ä» Yahoo Finance ä¸‹è½½æ•°æ®ï¼Œå¹¶è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ä½œä¸ºç‰¹å¾ã€‚
    """
    print(f"æ­£åœ¨ä¸‹è½½ {ticker} æ•°æ®...")
    try:
        # ä¸‹è½½è¶³å¤Ÿé•¿çš„æ—¶é—´è·¨åº¦ä»¥ç¡®ä¿è®¡ç®—å‡çº¿æ—¶ä¸äº§ç”Ÿç©ºå€¼
        df = yf.download(ticker, start='2019-10-01', end=None, progress=False)
    except Exception as e:
        print(f"ä¸‹è½½å¤±è´¥: {e}")
        sys.exit(1)
    
    if df.empty:
        print("æœªè·å–åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œã€‚")
        sys.exit(1)

    # å¤„ç†å¤šçº§ç´¢å¼•é—®é¢˜ (yfinance æ–°ç‰ˆç‰¹æ€§)
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.get_level_values(0)
        
    # ä¿ç•™åŸºç¡€ OHLCV æ•°æ®
    df = df[['Open', 'High', 'Low', 'Close', 'Volume']]
    
    # --- ç‰¹å¾å·¥ç¨‹ (Feature Engineering) ---
    # 1. ç§»åŠ¨å¹³å‡çº¿ (Trend)
    df['MA10'] = df['Close'].rolling(window=10).mean()
    df['MA20'] = df['Close'].rolling(window=20).mean()
    
    # 2. RSI (ç›¸å¯¹å¼ºå¼±æŒ‡æ•° - Momentum)
    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))

    # 3. MACD (è¶‹åŠ¿æŒ‡æ ‡)
    ema12 = df['Close'].ewm(span=12, adjust=False).mean()
    ema26 = df['Close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = ema12 - ema26
    df['Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()

    # 4. å¸ƒæ—å¸¦ (æ³¢åŠ¨ç‡æŒ‡æ ‡)
    df['BB_Upper'] = df['MA20'] + 2 * df['Close'].rolling(window=20).std()
    df['BB_Lower'] = df['MA20'] - 2 * df['Close'].rolling(window=20).std()
    
    # åˆ é™¤å› è®¡ç®—æŒ‡æ ‡äº§ç”Ÿçš„ NaN è¡Œ (å‰20-30è¡Œ)
    df = df.dropna()
    
    # æœ€ç»ˆé€‰ç”¨çš„ç‰¹å¾åˆ—
    feature_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 
                    'MA10', 'MA20', 'RSI', 'MACD', 'Signal', 'BB_Upper', 'BB_Lower']
    df = df[feature_cols]
    
    print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆã€‚å½“å‰ç‰¹å¾æ•°: {df.shape[1]}")
    return df

# ==============================================================================
# 2. æ•°æ®é›†å¤„ç† (åˆ‡åˆ†ã€å½’ä¸€åŒ–ã€æ—¶é—´çª—æ„å»º)
# ==============================================================================
def split_and_scale(df: pd.DataFrame, look_back: int) -> tuple[np.ndarray, np.ndarray, MinMaxScaler, pd.DataFrame]:
    """
    å°†æ•°æ®æŒ‰æ—¶é—´ä¸¥æ ¼åˆ‡åˆ†ä¸ºè®­ç»ƒé›†(2020-2024)å’Œæµ‹è¯•é›†(2025)ï¼Œå¹¶è¿›è¡Œå½’ä¸€åŒ–ã€‚
    æ³¨æ„ï¼šScaler åªèƒ½åœ¨è®­ç»ƒé›†ä¸Š fitï¼Œé˜²æ­¢æ•°æ®æ³„æ¼ã€‚
    """
    train_df = df.loc['2020-01-01':'2024-12-31']
    test_df_raw = df.loc['2025-01-01':]

    if len(test_df_raw) == 0:
        print("é”™è¯¯: 2025å¹´æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•ã€‚")
        sys.exit(1)

    # ä¸ºäº†è®©æµ‹è¯•é›†ç¬¬ä¸€å¤©æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®ï¼Œéœ€è¦æ‹¼æ¥è®­ç»ƒé›†æœ«å°¾çš„æ•°æ®
    full_dataset = pd.concat((train_df, test_df_raw), axis=0)
    test_inputs = full_dataset[len(full_dataset) - len(test_df_raw) - look_back:].values
    
    # å½’ä¸€åŒ– (0~1ä¹‹é—´)
    scaler = MinMaxScaler(feature_range=(0, 1))
    train_scaled = scaler.fit_transform(train_df.values) # åªåœ¨è®­ç»ƒé›†ä¸Šå­¦ä¹ å‚æ•°
    test_inputs_scaled = scaler.transform(test_inputs)   # æµ‹è¯•é›†åº”ç”¨ç›¸åŒå‚æ•°
    
    return train_scaled, test_inputs_scaled, scaler, test_df_raw

def create_xy(dataset: np.ndarray, look_back: int) -> tuple[np.ndarray, np.ndarray]:
    """
    å°†æ—¶é—´åºåˆ—æ•°æ®è½¬æ¢ä¸º LSTM éœ€è¦çš„ç›‘ç£å­¦ä¹ æ ¼å¼ (X, Y)ã€‚
    è¾“å…¥: è¿‡å» look_back å¤©çš„æ‰€æœ‰ç‰¹å¾
    è¾“å‡º: å½“å¤©çš„ High å’Œ Close
    """
    X, Y = [], []
    for i in range(look_back, len(dataset)):
        # X: ä» i-look_back åˆ° i-1 çš„æ•°æ®
        X.append(dataset[i-look_back:i, :])
        # Y: ç¬¬ i å¤©çš„æ•°æ® (Highåœ¨ç´¢å¼•1, Closeåœ¨ç´¢å¼•3)
        Y.append([dataset[i, 1], dataset[i, 3]]) 
    return np.array(X), np.array(Y)

# ==============================================================================
# 3. æ¨¡å‹æ„å»ºä¸é¢„æµ‹è¾…åŠ©
# ==============================================================================
def build_generic_lstm_model(layer_units: List[int], input_shape: Tuple[int, int]) -> Model:
    """
    åŠ¨æ€æ„å»º LSTM æ¨¡å‹ã€‚
    layer_units: åˆ—è¡¨ï¼Œä¾‹å¦‚ [64, 32] è¡¨ç¤ºä¸¤å±‚ LSTMï¼ŒèŠ‚ç‚¹æ•°åˆ†åˆ«ä¸º 64 å’Œ 32ã€‚
    """
    model = Sequential()
    for i, units in enumerate(layer_units):
        # å¦‚æœä¸æ˜¯æœ€åä¸€å±‚ LSTMï¼Œå¿…é¡»è®¾ç½® return_sequences=True ä»¥ä¼ é€’åºåˆ—ç»™ä¸‹ä¸€å±‚
        return_seq = (i < len(layer_units) - 1)
        
        if i == 0:
            # ç¬¬ä¸€å±‚éœ€è¦æŒ‡å®šè¾“å…¥å½¢çŠ¶
            model.add(LSTM(units=units, return_sequences=return_seq, input_shape=input_shape))
        else:
            model.add(LSTM(units=units, return_sequences=return_seq))
        
        # Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
        model.add(Dropout(0.3))
    
    # è¾“å‡ºå±‚: é¢„æµ‹ High å’Œ Close ä¸¤ä¸ªå€¼
    model.add(Dense(units=2))
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
    return model

# å­¦ä¹ ç‡è°ƒåº¦å™¨: å½“ loss ä¸å†ä¸‹é™æ—¶ï¼Œè‡ªåŠ¨å‡å°å­¦ä¹ ç‡
lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=0)

def inverse_transform_helper(preds: np.ndarray, scaler: MinMaxScaler, feature_count: int) -> tuple[np.ndarray, np.ndarray]:
    """
    åå½’ä¸€åŒ–å·¥å…·ã€‚å› ä¸º Scaler æ˜¯é’ˆå¯¹æ‰€æœ‰ç‰¹å¾è®­ç»ƒçš„ï¼Œæ‰€ä»¥éœ€è¦æ„å»ºä¸€ä¸ªå¡«å……çŸ©é˜µæ¥è¿˜åŸã€‚
    """
    dummy = np.zeros((len(preds), feature_count))
    # å°†é¢„æµ‹å€¼å¡«å› High (idx 1) å’Œ Close (idx 3) çš„ä½ç½®
    dummy[:, 1] = preds[:, 0]
    dummy[:, 3] = preds[:, 1]
    res = scaler.inverse_transform(dummy)
    return res[:, 1], res[:, 3]

def evaluate_predictions(real: np.ndarray, pred: np.ndarray) -> tuple[float, float]:
    """è®¡ç®—å‡æ–¹æ ¹è¯¯å·® (RMSE) å’Œ å¹³å‡ç»å¯¹è¯¯å·® (MAE)"""
    rmse = np.sqrt(mean_squared_error(real, pred))
    mae = mean_absolute_error(real, pred)
    return rmse, mae

def get_model_predictions(model: Model, X_data: np.ndarray, feature_count: int, scaler: MinMaxScaler) -> tuple[np.ndarray, np.ndarray]:
    """å°è£…é¢„æµ‹å’Œåå½’ä¸€åŒ–è¿‡ç¨‹"""
    preds = model.predict(X_data, verbose=0)
    return inverse_transform_helper(preds, scaler, feature_count)

# ==============================================================================
# 4. ä¸»ç¨‹åºé€»è¾‘ (å¤šè½®å®éªŒå¹³å‡ç‰ˆ)
# ==============================================================================
def main():
    # --- æ ¸å¿ƒå‚æ•° ---
    LOOK_BACK = 30
    EPOCHS = 100 
    BATCH_SIZE = 32
    N_ROUNDS = 5  # [æ–°å¢] æ¯ä¸ªå®éªŒæ¨¡å‹è¿è¡Œçš„æ¬¡æ•°ï¼Œç”¨äºå–å¹³å‡å€¼

    # --- å®éªŒé…ç½®å­—å…¸ ---
    # Key: å®éªŒåç§°
    # Value: LSTM å±‚ç»“æ„åˆ—è¡¨
    EXPERIMENTS = {
        "Exp1_Normal": [64],                 
        "Exp1_Narrow": [32],
        "Exp1_Wide":   [128],
        "Exp2_Narrow": [128, 64],  
        "Exp2_Wide":   [256, 128],
        "Exp2_Small":  [64, 32],          
        "Exp3_Deep":   [128, 64, 32],
    }

    # 1. å‡†å¤‡æ•°æ®
    df = get_and_prepare_data()
    NUM_FEATURES = df.shape[1] 
    
    train_scaled, test_inputs_scaled, scaler, test_df_target = split_and_scale(df, LOOK_BACK)
    X_train, y_train = create_xy(train_scaled, LOOK_BACK)
    X_test, y_test = create_xy(test_inputs_scaled, LOOK_BACK)

    real_close = test_df_target['Close'].values
    real_high = test_df_target['High'].values
    dates = test_df_target.index

    final_results_summary = [] # å­˜å‚¨æ‰€æœ‰å®éªŒçš„æœ€ç»ˆå¹³å‡ç»“æœ

    print(f"\n======== å¼€å§‹å®éªŒ (æ¯ä¸ªæ¨¡å‹è¿è¡Œ {N_ROUNDS} è½®å–å¹³å‡å€¼) ========")

    # 2. å¤–å±‚å¾ªç¯ï¼šéå†ä¸åŒçš„æ¨¡å‹ç»“æ„
    for exp_name, layers_config in EXPERIMENTS.items():
        print(f"\n>> [å®éªŒç»„]: {exp_name} ç»“æ„: {layers_config}")
        
        # ç”¨äºå­˜å‚¨ N_ROUNDS æ¬¡è¿è¡Œçš„ä¸´æ—¶æ•°æ®
        temp_maes = []
        temp_rmses = []
        # å­˜å‚¨æ¯æ¬¡é¢„æµ‹çš„åŸå§‹ä»·æ ¼æ•°ç»„ï¼Œæœ€åæ±‚å¹³å‡æ›²çº¿
        temp_pred_high_list = [] 
        temp_pred_close_list = []

        # 3. å†…å±‚å¾ªç¯ï¼šæ¯ä¸ªæ¨¡å‹è·‘ N_ROUNDS æ¬¡
        for i in range(N_ROUNDS):
            print(f"   - ç¬¬ {i+1}/{N_ROUNDS} æ¬¡è®­ç»ƒ...", end="", flush=True)
            
            # [å…³é”®] æ¯æ¬¡è®¾ç½®ä¸åŒçš„ç§å­ï¼Œç¡®ä¿åˆå§‹æƒé‡ä¸åŒ
            current_seed = BASE_SEED + i
            np.random.seed(current_seed)
            tf.random.set_seed(current_seed)
            tf.keras.backend.clear_session() # æ¸…ç†å†…å­˜
            
            # æ„å»ºå¹¶è®­ç»ƒæ¨¡å‹
            model = build_generic_lstm_model(layers_config, (X_train.shape[1], X_train.shape[2]))
            model.fit(
                X_train, y_train, 
                epochs=EPOCHS, 
                batch_size=BATCH_SIZE, 
                verbose=0, 
                validation_split=0.1,
                callbacks=[lr_schedule] 
            )
            
            # é¢„æµ‹
            p_high, p_close = get_model_predictions(model, X_test, NUM_FEATURES, scaler)
            rmse, mae = evaluate_predictions(real_close, p_close)
            
            # è®°å½•å•æ¬¡ç»“æœ
            temp_maes.append(mae)
            temp_rmses.append(rmse)
            temp_pred_high_list.append(p_high)
            temp_pred_close_list.append(p_close)
            
            print(f" å®Œæˆ. (MAE: {mae:.2f})")

        # 4. è®¡ç®—å¹³å‡ç»“æœ (Ensemble Averaging)
        avg_mae = np.mean(temp_maes)
        avg_rmse = np.mean(temp_rmses)
        # å°†5æ¬¡é¢„æµ‹çš„æ›²çº¿ï¼ˆæ•°ç»„ï¼‰åœ¨å‚ç›´æ–¹å‘å–å¹³å‡ï¼Œå¾—åˆ°ä¸€æ¡æ›´å¹³æ»‘çš„æ›²çº¿
        avg_pred_high = np.mean(np.array(temp_pred_high_list), axis=0)
        avg_pred_close = np.mean(np.array(temp_pred_close_list), axis=0)
        
        print(f"   >> {exp_name} å¹³å‡ MAE: {avg_mae:.4f} | å¹³å‡ RMSE: {avg_rmse:.4f}")
        
        final_results_summary.append({
            "Experiment": exp_name,
            "Structure": str(layers_config),
            "Avg_MAE": avg_mae,
            "Avg_RMSE": avg_rmse,
            "Pred_High": avg_pred_high,   # å­˜å‚¨å¹³å‡é¢„æµ‹æ›²çº¿
            "Pred_Close": avg_pred_close
        })

    # 5. ç»“æœæ’åºä¸å±•ç¤º
    results_df = pd.DataFrame(final_results_summary).sort_values(by="Avg_MAE")
    
    print("\n" + "="*60)
    print(f"æœ€ç»ˆå®éªŒæŠ¥å‘Š (æŒ‰ {N_ROUNDS} è½®å¹³å‡ MAE æ’åº)")
    print("="*60)
    print(results_df[["Experiment", "Structure", "Avg_MAE", "Avg_RMSE"]].to_string(index=False))
    
    # è·å–æœ€ä½³æ¨¡å‹æ•°æ®
    best_exp = results_df.iloc[0]
    best_name = best_exp["Experiment"]
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹æ–¹æ¡ˆ: {best_name} (å¹³å‡MAE: {best_exp['Avg_MAE']:.4f})")

    # 6. ç»˜å›¾ (ç»˜åˆ¶æœ€ä½³æ¨¡å‹çš„å¹³å‡é¢„æµ‹ç»“æœ)
    print(f"\næ­£åœ¨ç»˜åˆ¶æœ€ä½³æ¨¡å‹ ({best_name}) çš„å¹³å‡é¢„æµ‹å›¾è¡¨...")
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)

    # ç»˜åˆ¶æœ€é«˜ä»·
    ax1.set_title(f'ä¸Šè¯ 2025 æœ€é«˜ä»·é¢„æµ‹ ({best_name}, {N_ROUNDS}è½®å¹³å‡)', fontsize=14, fontproperties=FONT_PROP)
    ax1.plot(dates, real_high, label='å®é™…æœ€é«˜ä»·', color='#d62728', linewidth=2)
    ax1.plot(dates, best_exp["Pred_High"], label='é¢„æµ‹æœ€é«˜ä»·(å¹³å‡)', color='#1f77b4', linestyle='--', linewidth=1.5)
    ax1.legend(loc='upper left', prop=FONT_PROP)
    ax1.grid(True, alpha=0.3)
    ax1.set_ylabel('ä»·æ ¼', fontproperties=FONT_PROP)

    # ç»˜åˆ¶æ”¶ç›˜ä»·
    ax2.set_title(f'ä¸Šè¯ 2025 æ”¶ç›˜ä»·é¢„æµ‹ ({best_name}, {N_ROUNDS}è½®å¹³å‡)', fontsize=14, fontproperties=FONT_PROP)
    ax2.plot(dates, real_close, label='å®é™…æ”¶ç›˜ä»·', color='#2ca02c', linewidth=2)
    ax2.plot(dates, best_exp["Pred_Close"], label='é¢„æµ‹æ”¶ç›˜ä»·(å¹³å‡)', color='#ff7f0e', linestyle='--', linewidth=1.5)
    ax2.legend(loc='upper left', prop=FONT_PROP)
    ax2.grid(True, alpha=0.3)
    ax2.set_ylabel('ä»·æ ¼', fontproperties=FONT_PROP)
    ax2.set_xlabel('æ—¥æœŸ', fontproperties=FONT_PROP)

    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
    plt.gcf().autofmt_xdate()
    plt.tight_layout()
    
    SAVE_NAME = f'LSTM_Final_Result_{best_name}.png'
    plt.savefig(SAVE_NAME, dpi=300)
    print(f"âœ… å›¾è¡¨å·²ä¿å­˜è‡³: {SAVE_NAME}")

if __name__ == "__main__":
    main()