import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib
matplotlib.use('Agg') 
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import matplotlib.font_manager as fm
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
import sys
import os
import random
from typing import List, Tuple

# ==============================================================================
# 0. åŸºç¡€è®¾ç½®
# ==============================================================================
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("ğŸš€ä»¥æ­¤è®¾å¤‡è¿è¡Œ: MPS (Apple Silicon GPU)")
elif torch.cuda.is_available():
    device = torch.device("cuda")
    print("ğŸš€ä»¥æ­¤è®¾å¤‡è¿è¡Œ: CUDA (Nvidia GPU)")
else:
    device = torch.device("cpu")
    print("âš ï¸ä»¥æ­¤è®¾å¤‡è¿è¡Œ: CPU")

BASE_SEED = 42

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(BASE_SEED)

# --- å­—ä½“è®¾ç½® ---
FONT_PROP = None
FONT_NAME = 'sans-serif'
CANDIDATE_FONTS = [
    "/System/Library/Fonts/STHeiti Light.ttf",
    "/System/Library/Fonts/Supplemental/Arial Unicode.ttf",
    "/Library/Fonts/Arial Unicode.ttf"
]
try:
    found_font = False
    for path in CANDIDATE_FONTS:
        if os.path.exists(path):
            FONT_PROP = fm.FontProperties(fname=path, size=12)
            FONT_NAME = FONT_PROP.get_name()
            plt.rcParams['font.family'] = 'sans-serif'
            plt.rcParams['font.sans-serif'] = [FONT_NAME]
            plt.rcParams['axes.unicode_minus'] = False
            found_font = True
            break
except Exception:
    pass
plt.style.use('seaborn-v0_8') 

# ==============================================================================
# 1. æ•°æ®è·å–ä¸ç‰¹å¾å·¥ç¨‹ (æ ¸å¿ƒä¿®æ”¹ï¼šçº¯ç›¸å¯¹å€¼ + é˜ˆå€¼è¿‡æ»¤)
# ==============================================================================
def get_and_prepare_data(ticker: str = '000001.SS') -> pd.DataFrame:
    print(f"æ­£åœ¨ä¸‹è½½ {ticker} æ•°æ®...")
    try:
        # [ä¿®æ”¹]: ä½¿ç”¨ 2015 è‡³ä»Šçš„æ•°æ®
        df = yf.download(ticker, start='2015-01-01', end=None, progress=False)
    except Exception as e:
        print(f"ä¸‹è½½å¤±è´¥: {e}")
        sys.exit(1)
    
    if df.empty: sys.exit(1)
    if isinstance(df.columns, pd.MultiIndex): df.columns = df.columns.get_level_values(0)
    df = df[['Open', 'High', 'Low', 'Close', 'Volume']]
    
    # --- ç‰¹å¾å·¥ç¨‹ (ä¿æŒç›¸å¯¹å€¼é€»è¾‘) ---
    df['Log_Ret_Close'] = np.log(df['Close'] / df['Close'].shift(1) + 1e-8)
    df['Log_Ret_Open']  = np.log(df['Open'] / df['Open'].shift(1) + 1e-8)
    df['Log_Ret_High']  = np.log(df['High'] / df['High'].shift(1) + 1e-8)
    df['Log_Ret_Low']   = np.log(df['Low'] / df['Low'].shift(1) + 1e-8)
    df['Log_Ret_Vol']   = np.log(df['Volume'] / df['Volume'].shift(1).replace(0, 1))

    ma10 = df['Close'].rolling(window=10).mean()
    df['MA10_Bias'] = (df['Close'] - ma10) / ma10
    ma20 = df['Close'].rolling(window=20).mean()
    df['MA20_Bias'] = (df['Close'] - ma20) / ma20
    ma60 = df['Close'].rolling(window=60).mean()
    df['MA60_Bias'] = (df['Close'] - ma60) / ma60

    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))

    ema12 = df['Close'].ewm(span=12, adjust=False).mean()
    ema26 = df['Close'].ewm(span=26, adjust=False).mean()
    macd_raw = ema12 - ema26
    signal_raw = macd_raw.ewm(span=9, adjust=False).mean()
    df['MACD_Norm'] = macd_raw / df['Close']
    df['Signal_Norm'] = signal_raw / df['Close']

    rolling_mean = df['Close'].rolling(window=20).mean()
    rolling_std = df['Close'].rolling(window=20).std()
    upper = rolling_mean + 2 * rolling_std
    lower = rolling_mean - 2 * rolling_std
    df['BB_PctB'] = (df['Close'] - lower) / (upper - lower)
    df['BB_Width'] = (upper - lower) / rolling_mean

    # --- é¢„æµ‹ç›®æ ‡ ---
    df['Price_Change'] = df['Close'].pct_change().shift(-1)
    
    # é˜ˆå€¼è¿‡æ»¤
    THRESHOLD = 0.002
    df_filtered = df[abs(df['Price_Change']) > THRESHOLD].copy()
    df_filtered['Target_Direction'] = (df_filtered['Price_Change'] > 0).astype(np.float32)

    df_filtered = df_filtered.dropna()
    
    # [å…³é”®ä¿®æ”¹]: æˆ‘ä»¬æŠŠ Price_Change ä¹ŸåŠ è¿› feature_colsï¼Œä½†æ”¾åœ¨ Target åé¢
    # è¿™æ ·å®ƒä¸ä¼šå½±å“è®­ç»ƒ (æˆ‘ä»¬ä¼šåœ¨ split æ—¶æŠŠå®ƒåˆ‡æ‰)ï¼Œä½†æ–¹ä¾¿åç»­å›æµ‹æå–
    feature_cols = [
        'Log_Ret_Close', 'Log_Ret_Open', 'Log_Ret_High', 'Log_Ret_Low', 'Log_Ret_Vol',
        'MA10_Bias', 'MA20_Bias', 'MA60_Bias',
        'RSI', 'MACD_Norm', 'Signal_Norm',
        'BB_PctB', 'BB_Width',
        'Target_Direction', # å€’æ•°ç¬¬äºŒåˆ—ï¼šæ ‡ç­¾
        'Price_Change'      # æœ€åä¸€åˆ—ï¼šçœŸå®æ¶¨è·Œå¹… (ç”¨äºå›æµ‹ï¼Œä¸ç”¨äºè®­ç»ƒ)
    ]
    
    final_df = df_filtered[feature_cols]
    
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆã€‚åŒ…å«å›æµ‹æ•°æ®åˆ—ã€‚")
    return final_df

# ==============================================================================
# 2. æ•°æ®é›†å¤„ç†
# ==============================================================================
def split_and_scale(df: pd.DataFrame, look_back: int) -> tuple:
    train_df = df.loc[:'2024-12-31'] # åŠ¨æ€åˆ‡åˆ†
    test_df_raw = df.loc['2025-01-01':]

    if len(train_df) == 0 or len(test_df_raw) == 0:
        sys.exit(1)

    # æ‹¼æ¥å†å²çª—å£
    full_dataset = pd.concat((train_df, test_df_raw), axis=0)
    # è¿™é‡Œçš„ .values åŒ…å«äº†æ‰€æœ‰åˆ—ï¼ŒåŒ…æ‹¬ Target å’Œ Price_Change
    test_inputs = full_dataset[len(full_dataset) - len(test_df_raw) - look_back:].values
    
    scaler = StandardScaler()
    
    # [å…³é”®ä¿®æ”¹]: è®­ç»ƒé›†ç¼©æ”¾
    # æˆ‘ä»¬åªç¼©æ”¾ç‰¹å¾åˆ— (å³æ’é™¤æœ€åä¸¤åˆ— Target_Direction å’Œ Price_Change)
    # iloc[:, :-2] å–é™¤äº†æœ€åä¸¤åˆ—ä¹‹å¤–çš„æ‰€æœ‰åˆ—
    X_train_scaled = scaler.fit_transform(train_df.iloc[:, :-2].values)
    
    # y_train å–å€’æ•°ç¬¬äºŒåˆ— (Target_Direction)
    y_train = train_df.iloc[:, -2].values.reshape(-1, 1)

    # æµ‹è¯•é›†ç¼©æ”¾ (åŒæ ·åªç¼©æ”¾ç‰¹å¾åˆ—)
    X_test_inputs_scaled = scaler.transform(test_inputs[:, :-2])
    
    # æ‹¼å›å»ï¼šç¼©æ”¾åçš„X + åŸå§‹Target + åŸå§‹Price_Change
    test_inputs_scaled = np.hstack([
        X_test_inputs_scaled, 
        test_inputs[:, -2].reshape(-1, 1), # Target
        test_inputs[:, -1].reshape(-1, 1)  # Price_Change
    ])

    return X_train_scaled, y_train, test_inputs_scaled, scaler, test_df_raw

def create_xy(X_data: np.ndarray, y_data: np.ndarray, look_back: int):
    X, Y = [], []
    for i in range(look_back, len(X_data)):
        X.append(X_data[i-look_back:i, :])
        Y.append([y_data[i, 0]]) 
    return np.array(X), np.array(Y)

# ==============================================================================
# 3. æ¨¡å‹æ„å»º
# ==============================================================================
class LSTMClassifier(nn.Module):
    def __init__(self, input_size, hidden_layer_sizes, output_size=1, dropout_prob=0.3):
        super(LSTMClassifier, self).__init__()
        self.layers = nn.ModuleList()
        self.dropouts = nn.ModuleList()
        
        prev_size = input_size
        for hidden_size in hidden_layer_sizes:
            self.layers.append(nn.LSTM(prev_size, hidden_size, batch_first=True))
            self.dropouts.append(nn.Dropout(dropout_prob))
            prev_size = hidden_size
            
        self.fc = nn.Linear(prev_size, output_size)

    def forward(self, x):
        out = x
        for i in range(len(self.layers)):
            out, _ = self.layers[i](out)
            out = self.dropouts[i](out)
        out = out[:, -1, :] 
        out = self.fc(out)
        return out

# ==============================================================================
# 4. è¯„ä¼°é€»è¾‘
# ==============================================================================
def evaluate_predictions(real_labels: np.ndarray, pred_logits: np.ndarray) -> float:
    pred_probs = 1.0 / (1.0 + np.exp(-pred_logits))
    pred_labels = (pred_probs >= 0.5).astype(int)
    accuracy = accuracy_score(real_labels, pred_labels)
    return accuracy

def run_backtest(model, X_test_tensor, test_returns, dates):
    """
    å›æµ‹å‡½æ•°
    model: è®­ç»ƒå¥½çš„æ¨¡å‹
    X_test_tensor: æµ‹è¯•é›†è¾“å…¥ç‰¹å¾
    test_returns: æµ‹è¯•é›†æ¯å¤©çš„çœŸå®æ¶¨è·Œå¹… (Price_Change)
    dates: æµ‹è¯•é›†æ—¥æœŸ
    """
    model.eval()
    with torch.no_grad():
        logits = model(X_test_tensor)
        probs = 1.0 / (1.0 + np.exp(-logits.cpu().numpy()))
        # ç”Ÿæˆä¿¡å·ï¼š1=ä¹°å…¥/æŒæœ‰ï¼Œ0=ç©ºä»“/å–å‡º
        signals = (probs >= 0.5).astype(int).flatten()
    
    # è®¡ç®—ç­–ç•¥æ”¶ç›Š
    # ç­–ç•¥é€»è¾‘ï¼šå¦‚æœé¢„æµ‹æ¶¨(1)ï¼Œåˆ™è·å¾—å½“å¤©çš„ Price_Changeï¼›å¦‚æœé¢„æµ‹è·Œ(0)ï¼Œæ”¶ç›Šä¸º0
    strategy_returns = signals * test_returns
    
    # è®¡ç®—èµ„é‡‘æ›²çº¿ (Cumulative Returns)
    # åˆå§‹èµ„é‡‘è®¾ä¸º 1
    cumulative_market = np.cumprod(1 + test_returns)
    cumulative_strategy = np.cumprod(1 + strategy_returns)
    
    # è®¡ç®—æœ€ç»ˆæ”¶ç›Šç‡
    total_return_market = cumulative_market[-1] - 1
    total_return_strategy = cumulative_strategy[-1] - 1
    
    print("\n" + "="*40)
    print("ğŸ’° å›æµ‹æŠ¥å‘Š (Backtest Report)")
    print(f"å¸‚åœºåŸºå‡†æ”¶ç›Šç‡: {total_return_market:.2%}")
    print(f"LSTM ç­–ç•¥æ”¶ç›Šç‡: {total_return_strategy:.2%}")
    if total_return_strategy > total_return_market:
        print("ğŸ‰ æ­å–œï¼ç­–ç•¥è·‘èµ¢äº†å¸‚åœºï¼")
    else:
        print("ğŸ¥€ é—æ†¾ï¼Œç­–ç•¥æ²¡è·‘èµ¢å¸‚åœºã€‚")
    print("="*40)

    # ç»˜å›¾
    plt.figure(figsize=(12, 6))
    plt.plot(dates, cumulative_market, label=f'Market Benchmark ({total_return_market:.2%})', color='gray', alpha=0.5, linestyle='--')
    plt.plot(dates, cumulative_strategy, label=f'LSTM Strategy ({total_return_strategy:.2%})', color='red', linewidth=2)
    
    plt.title('Equity Curve: LSTM Strategy vs Market (2025)', fontsize=14, fontproperties=FONT_PROP)
    plt.ylabel('Normalized Value (Start=1)')
    plt.legend(loc='upper left', prop=FONT_PROP)
    plt.grid(True, alpha=0.3)
    
    # æ ‡è®°ä¹°å–ç‚¹ (å¯é€‰ï¼Œä¸ºäº†ä¸è®©å›¾å¤ªä¹±ï¼Œåªæ ‡è¿™ä¸€è¡Œ)
    # plt.scatter(dates[signals==1], cumulative_strategy[signals==1], marker='^', color='g', s=10, alpha=0.6)
    
    plt.gcf().autofmt_xdate()
    plt.savefig('Backtest_Result.png', dpi=300)
    print("âœ… èµ„é‡‘æ›²çº¿å›¾å·²ä¿å­˜è‡³: Backtest_Result.png") 
# ==============================================================================
# 5. ä¸»ç¨‹åºé€»è¾‘
# ==============================================================================
def main():
    LOOK_BACK = 15 # ä½ ä¹‹å‰ç”¨çš„15
    EPOCHS = 80
    BATCH_SIZE = 512
    N_ROUNDS = 5 

    # ä½¿ç”¨ä½ æ•ˆæœæœ€å¥½çš„ç»“æ„
    EXPERIMENTS = {
        "Exp2_1": [64, 32],
    }

    # 1. æ•°æ®å‡†å¤‡ (åˆ—æ•°å˜äº†ï¼ŒTotal features è¦å‡å»æœ€åä¸¤åˆ—)
    df = get_and_prepare_data()
    TOTAL_FEATURES = df.shape[1] - 2 
    
    X_train_scaled, y_train_np, test_inputs_scaled, scaler, test_df_target = split_and_scale(df, LOOK_BACK)
    
    # åˆ›å»ºè®­ç»ƒé›†
    X_train_np, y_train_np_window = create_xy(X_train_scaled, y_train_np, LOOK_BACK)
    
    # åˆ›å»ºæµ‹è¯•é›†
    # [å…³é”®ä¿®æ”¹]: test_inputs_scaled ç°åœ¨çš„åˆ—ç»“æ„æ˜¯ [ç‰¹å¾..., Target, Price_Change]
    # ç‰¹å¾éƒ¨åˆ†: [:, :-2]
    # æ ‡ç­¾éƒ¨åˆ†: [:, -2]
    # æ”¶ç›Šç‡éƒ¨åˆ†: [:, -1]
    
    X_test_np, y_test_np = create_xy(test_inputs_scaled[:, :-2], 
                                     test_inputs_scaled[:, -2].reshape(-1, 1), 
                                     LOOK_BACK)
    
    # æå–å›æµ‹ç”¨çš„çœŸå®æ”¶ç›Šç‡ (å¯¹åº” X_test çš„æ—¶é—´æ®µ)
    # å› ä¸º create_xy ä¼šä» LOOK_BACK å¼€å§‹æˆªå–ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿä» LOOK_BACK å¼€å§‹æˆªå–æ”¶ç›Šç‡
    test_returns_raw = test_inputs_scaled[LOOK_BACK:, -1]

    # è½¬ Tensor
    X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32).to(device)
    y_train_tensor = torch.tensor(y_train_np_window, dtype=torch.float32).to(device)
    X_test_tensor = torch.tensor(X_test_np, dtype=torch.float32).to(device)
    y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32).to(device)
    
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

    real_labels_np = y_test_np.flatten()
    
    # æœ€ä½³æ¨¡å‹ä¿å­˜é€»è¾‘
    best_acc = 0.0
    best_model_path = "best_model.pth"

    print(f"\n======== å¼€å§‹ PyTorch åˆ†ç±»å®éªŒ ========")

    for exp_name, layers_config in EXPERIMENTS.items():
        print(f"\n>> [å®éªŒç»„]: {exp_name} ç»“æ„: {layers_config}")
        
        for i in range(N_ROUNDS):
            # ... (è®­ç»ƒä»£ç ä¿æŒä¸å˜) ...
            set_seed(BASE_SEED + i)
            model = LSTMClassifier(input_size=TOTAL_FEATURES, hidden_layer_sizes=layers_config, output_size=1).to(device)
            criterion = nn.BCEWithLogitsLoss()
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)

            for epoch in range(EPOCHS):
                model.train()
                for batch_X, batch_y in train_loader:
                    optimizer.zero_grad()
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    optimizer.step()
                model.eval()
                with torch.no_grad():
                    val_loss = criterion(model(X_test_tensor), y_test_tensor).item()
                scheduler.step(val_loss)

            # è¯„ä¼°
            model.eval()
            with torch.no_grad():
                pred_logits = model(X_test_tensor).cpu().numpy().flatten()
            
            acc = evaluate_predictions(real_labels_np, pred_logits)
            print(f"   Round {i+1}: Acc {acc:.4f}")
            
            # ä¿å­˜æœ€ä½³
            if acc > best_acc:
                best_acc = acc
                torch.save(model.state_dict(), best_model_path)

    # --- ğŸŒŸ æ ¸å¿ƒå›æµ‹ç¯èŠ‚ ---
    print("\n======== å¼€å§‹å›æµ‹ (Backtesting) ========")
    # 1. é‡æ–°åŠ è½½æœ€ä½³æ¨¡å‹
    best_model = LSTMClassifier(input_size=TOTAL_FEATURES, hidden_layer_sizes=EXPERIMENTS["Exp2_1"], output_size=1).to(device)
    best_model.load_state_dict(torch.load(best_model_path))
    
    # 2. è¿è¡Œå›æµ‹
    # dates éœ€è¦å¯¹é½ (ä» LOOK_BACK å¼€å§‹)
    dates = test_df_target.index
    run_backtest(best_model, X_test_tensor, test_returns_raw, dates)

if __name__ == "__main__":
    test_df_raw = [] 
    main()   