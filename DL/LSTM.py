import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib
# [è®¾ç½®]: å¼ºåˆ¶ä½¿ç”¨éäº¤äº’å¼åç«¯ Agg (ä¿å­˜å›¾ç‰‡ä¸“ç”¨)
matplotlib.use('Agg') 
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import matplotlib.font_manager as fm
import tensorflow as tf
# [ä¿®æ”¹]: å¼•å…¥ StandardScaler (å¯¹æ”¶ç›Šç‡è¿™ç§æ­£æ€åˆ†å¸ƒæ•°æ®ï¼ŒStandardScaler æ¯” MinMaxScaler æ›´å¥½)
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau
import sys
import os
from typing import List, Tuple

# ==============================================================================
# 0. åŸºç¡€è®¾ç½® (éšæœºç§å­ & å­—ä½“é…ç½®)
# ==============================================================================
BASE_SEED = 42
os.environ['PYTHONHASHSEED'] = str(BASE_SEED)

# --- å­—ä½“è®¾ç½® ---
FONT_PROP = None
FONT_NAME = 'sans-serif'
CANDIDATE_FONTS = [
    "/System/Library/Fonts/STHeiti Light.ttf",
    "/System/Library/Fonts/Supplemental/Arial Unicode.ttf",
    "/Library/Fonts/Arial Unicode.ttf"
]

try:
    found_font = False
    for path in CANDIDATE_FONTS:
        if os.path.exists(path):
            FONT_PROP = fm.FontProperties(fname=path, size=12)
            FONT_NAME = FONT_PROP.get_name()
            plt.rcParams['font.family'] = 'sans-serif'
            plt.rcParams['font.sans-serif'] = [FONT_NAME]
            plt.rcParams['axes.unicode_minus'] = False
            print(f"âœ… å­—ä½“é…ç½®æˆåŠŸ: {FONT_NAME}")
            found_font = True
            break
    if not found_font:
        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
except Exception as e:
    print(f"âŒ å­—ä½“é…ç½®å¼‚å¸¸: {e}")

tf.keras.backend.clear_session()
plt.style.use('seaborn-v0_8') 

# ==============================================================================
# 1. æ•°æ®è·å–ä¸ç‰¹å¾å·¥ç¨‹ (å…³é”®ä¿®æ”¹ï¼šè®¡ç®—æ”¶ç›Šç‡)
# ==============================================================================
def get_and_prepare_data(ticker: str = '000001.SS') -> pd.DataFrame:
    print(f"æ­£åœ¨ä¸‹è½½ {ticker} æ•°æ®...")
    try:
        df = yf.download(ticker, start='2019-01-01', end=None, progress=False)
    except Exception as e:
        print(f"ä¸‹è½½å¤±è´¥: {e}")
        sys.exit(1)
    
    if df.empty: sys.exit(1)
    if isinstance(df.columns, pd.MultiIndex): df.columns = df.columns.get_level_values(0)
    df = df[['Open', 'High', 'Low', 'Close', 'Volume']]
    
    # --- æŠ€æœ¯æŒ‡æ ‡ (ä½œä¸ºè¾“å…¥ç‰¹å¾ X) ---
    df['MA10'] = df['Close'].rolling(window=10).mean()
    df['MA20'] = df['Close'].rolling(window=20).mean()
    
    # RSI
    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))

    # MACD
    ema12 = df['Close'].ewm(span=12, adjust=False).mean()
    ema26 = df['Close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = ema12 - ema26
    df['Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()

    # å¸ƒæ—å¸¦
    df['BB_Upper'] = df['MA20'] + 2 * df['Close'].rolling(window=20).std()
    df['BB_Lower'] = df['MA20'] - 2 * df['Close'].rolling(window=20).std()
    
    # --- [å…³é”®ä¿®æ”¹] è®¡ç®—é¢„æµ‹ç›®æ ‡ (ä½œä¸ºè¾“å‡º Y) ---
    # ä½¿ç”¨å¯¹æ•°æ”¶ç›Šç‡: ln(Today / Yesterday)
    # 1. æ”¶ç›˜ä»·æ”¶ç›Šç‡
    df['Log_Ret_Close'] = np.log(df['Close'] / df['Close'].shift(1))
    # 2. æœ€é«˜ä»·æ”¶ç›Šç‡ (å®šä¹‰ä¸º: å½“æ—¥æœ€é«˜ä»· ç›¸å¯¹äº æ˜¨æ—¥æœ€é«˜ä»· çš„å˜åŒ–)
    # æ³¨æ„ï¼šä¹Ÿå¯ä»¥å®šä¹‰ä¸ºç›¸å¯¹äºæ˜¨æ—¥æ”¶ç›˜ä»·ï¼Œè¿™é‡Œä¿æŒé€»è¾‘ä¸€è‡´æ€§
    df['Log_Ret_High'] = np.log(df['High'] / df['High'].shift(1))

    # æ¸…é™¤è®¡ç®—äº§ç”Ÿçš„ç©ºå€¼
    df = df.dropna()
    
    # æ•´ç†åˆ—é¡ºåº:
    # å‰é¢æ˜¯è¾“å…¥ç‰¹å¾(X)ï¼Œæœ€åä¸¤åˆ—æ˜¯é¢„æµ‹ç›®æ ‡(Y)
    feature_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 
                    'MA10', 'MA20', 'RSI', 'MACD', 'Signal', 'BB_Upper', 'BB_Lower',
                    'Log_Ret_High', 'Log_Ret_Close'] # <--- ç›®æ ‡åœ¨æœ€å
    df = df[feature_cols]
    
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆã€‚ç‰¹å¾æ•°: {df.shape[1]-2}, ç›®æ ‡æ•°: 2 (æ”¶ç›Šç‡)")
    return df

# ==============================================================================
# 2. æ•°æ®é›†å¤„ç†
# ==============================================================================
def split_and_scale(df: pd.DataFrame, look_back: int) -> tuple[np.ndarray, np.ndarray, StandardScaler, pd.DataFrame]:
    # åˆ‡åˆ†æ—¶é—´
    train_df = df.loc['2020-01-01':'2024-12-31']
    test_df_raw = df.loc['2025-01-01':]

    if len(test_df_raw) == 0: sys.exit(1)

    # æ‹¼æ¥æµ‹è¯•é›†æ‰€éœ€çš„å†å²çª—å£
    full_dataset = pd.concat((train_df, test_df_raw), axis=0)
    test_inputs = full_dataset[len(full_dataset) - len(test_df_raw) - look_back:].values
    
    # [ä¿®æ”¹]: ä½¿ç”¨ StandardScaler
    # åŸå› ï¼šæ”¶ç›Šç‡æ•°æ®é€šå¸¸æ¥è¿‘æ­£æ€åˆ†å¸ƒï¼ˆé’Ÿå½¢æ›²çº¿ï¼‰ï¼ŒStandardScaler æ¯” MinMax æ•ˆæœæ›´å¥½
    scaler = StandardScaler()
    train_scaled = scaler.fit_transform(train_df.values)
    test_inputs_scaled = scaler.transform(test_inputs)
    
    return train_scaled, test_inputs_scaled, scaler, test_df_raw

def create_xy(dataset: np.ndarray, look_back: int) -> tuple[np.ndarray, np.ndarray]:
    X, Y = [], []
    # dataset æœ€åä¸€åˆ—æ˜¯ Closeæ”¶ç›Šç‡(-1), å€’æ•°ç¬¬äºŒåˆ—æ˜¯ Highæ”¶ç›Šç‡(-2)
    idx_high_ret = -2
    idx_close_ret = -1
    
    for i in range(look_back, len(dataset)):
        # è¾“å…¥ X: ä½¿ç”¨æ‰€æœ‰ç‰¹å¾ (åŒ…æ‹¬è¿‡å»å‡ å¤©çš„æ”¶ç›Šç‡ï¼Œè¿™å¯¹é¢„æµ‹å¾ˆæœ‰å¸®åŠ©)
        X.append(dataset[i-look_back:i, :])
        
        # è¾“å‡º Y: é¢„æµ‹å½“å¤©çš„ [High_Ret, Close_Ret]
        Y.append([dataset[i, idx_high_ret], dataset[i, idx_close_ret]]) 
        
    return np.array(X), np.array(Y)

# ==============================================================================
# 3. æ¨¡å‹æ„å»º
# ==============================================================================
def build_generic_lstm_model(layer_units: List[int], input_shape: Tuple[int, int]) -> Model:
    model = Sequential()
    model.add(Input(shape=input_shape))
    
    for i, units in enumerate(layer_units):
        return_seq = (i < len(layer_units) - 1)
        model.add(LSTM(units=units, return_sequences=return_seq))
        model.add(Dropout(0.3))
    
    model.add(Dense(units=2)) # è¾“å‡ºå±‚: é¢„æµ‹2ä¸ªæ”¶ç›Šç‡å€¼
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
    return model

lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=0)

# ==============================================================================
# 4. æ ¸å¿ƒé€»è¾‘: ä»·æ ¼è¿˜åŸ (Reconstruction)
# ==============================================================================
def recover_prices(pred_returns_scaled: np.ndarray, scaler: StandardScaler, 
                   prev_prices_high: np.ndarray, prev_prices_close: np.ndarray, 
                   feature_total_count: int) -> tuple[np.ndarray, np.ndarray]:
    """
    å°†æ¨¡å‹é¢„æµ‹çš„ã€å½’ä¸€åŒ–æ”¶ç›Šç‡ã€‘è¿˜åŸä¸ºã€çœŸå®ä»·æ ¼ç‚¹ä½ã€‘
    å…¬å¼: ä»Šæ—¥ä»·æ ¼ = æ˜¨æ—¥ä»·æ ¼ * exp(ä»Šæ—¥é¢„æµ‹å¯¹æ•°æ”¶ç›Šç‡)
    """
    # 1. åå½’ä¸€åŒ– (Inverse Scale)
    # æ„é€ å¡«å……çŸ©é˜µï¼Œå› ä¸º scaler æ˜¯å¯¹æ‰€æœ‰åˆ—è®­ç»ƒçš„
    dummy = np.zeros((len(pred_returns_scaled), feature_total_count))
    # å°†é¢„æµ‹å€¼å¡«å…¥å¯¹åº”çš„æ”¶ç›Šç‡åˆ—ä½ç½® (æœ€åä¸¤åˆ—)
    dummy[:, -2] = pred_returns_scaled[:, 0] # High Ret
    dummy[:, -1] = pred_returns_scaled[:, 1] # Close Ret
    
    # åè½¬ scaling
    res_unscaled = scaler.inverse_transform(dummy)
    
    # æå–çœŸå®çš„å¯¹æ•°æ”¶ç›Šç‡
    pred_log_ret_high = res_unscaled[:, -2]
    pred_log_ret_close = res_unscaled[:, -1]
    
    # 2. ä»·æ ¼è¿˜åŸ (Price Reconstruction)
    # é¢„æµ‹ä»·æ ¼ = æ˜¨æ—¥ä»·æ ¼ * exp(é¢„æµ‹çš„å¯¹æ•°æ”¶ç›Šç‡)
    rec_high = prev_prices_high * np.exp(pred_log_ret_high)
    rec_close = prev_prices_close * np.exp(pred_log_ret_close)
    
    return rec_high, rec_close

def evaluate_predictions(real: np.ndarray, pred: np.ndarray) -> tuple[float, float]:
    rmse = np.sqrt(mean_squared_error(real, pred))
    mae = mean_absolute_error(real, pred)
    return rmse, mae

# ==============================================================================
# 5. ä¸»ç¨‹åºé€»è¾‘
# ==============================================================================
def main():
    LOOK_BACK = 30
    EPOCHS = 80
    BATCH_SIZE = 32
    N_ROUNDS = 5 # 5è½®å¹³å‡

    EXPERIMENTS = {
        "Exp1_Normal": [64],                 
        "Exp1_Wide":   [128],
        "Exp2_Normal": [128, 64],  
        "Exp2_Narrow": [64, 32],
        "Exp2_Wide": [256, 128],
        "Exp2_Same":   [128, 128],
    }

    # 1. æ•°æ®å‡†å¤‡
    df = get_and_prepare_data()
    TOTAL_FEATURES = df.shape[1]
    
    train_scaled, test_inputs_scaled, scaler, test_df_target = split_and_scale(df, LOOK_BACK)
    X_train, y_train = create_xy(train_scaled, LOOK_BACK)
    X_test, y_test = create_xy(test_inputs_scaled, LOOK_BACK)

    # çœŸå®ä»·æ ¼ (ç”¨äºè¯„ä¼°)
    real_close = test_df_target['Close'].values
    real_high = test_df_target['High'].values
    dates = test_df_target.index
    
    # [å…³é”®]: è·å–æµ‹è¯•é›†æ¯ä¸€å¤©å¯¹åº”çš„"å‰ä¸€å¤©ä»·æ ¼" (ç”¨äºä»æ”¶ç›Šç‡è¿˜åŸä»·æ ¼)
    # test_df_target æ˜¯2025å¹´çš„æ•°æ®ã€‚
    # å®ƒçš„ç¬¬ i å¤©çš„åŸºå‡†ä»·æ ¼ï¼Œåº”è¯¥æ˜¯ç¬¬ i-1 å¤©çš„ä»·æ ¼ã€‚
    # æˆ‘ä»¬éœ€è¦æŠŠæ•´ä¸ªåºåˆ—å‘ä¸‹ç§»åŠ¨ä¸€ä½ï¼Œç¬¬ä¸€å¤©çš„åŸºå‡†éœ€è¦å»å†å²æ•°æ®é‡Œæ‰¾ï¼ˆsplitæ—¶å·²å¤„ç†è¿ç»­æ€§ï¼Œä½†åœ¨pandasé‡Œæ“ä½œæ›´æ–¹ä¾¿ï¼‰
    
    # è·å–åŒ…å«2024æœ€åä¸€å¤©çš„æ•°æ®ä»¥ä¾¿ shift
    full_target_prices = pd.concat([
        df.loc['2024-12-01':].iloc[-(len(test_df_raw)+1):], # å–è¶³å¤Ÿé•¿ï¼Œåªè¦æœ€ålen+1ä¸ª
    ])
    # å®é™…ä¸Šï¼Œtest_df_target['Close'].shift(1) ä¼šå¯¼è‡´ç¬¬ä¸€å¤©æ˜¯ NaN
    # æˆ‘ä»¬éœ€è¦å®Œæ•´çš„ä»·æ ¼åºåˆ—æ¥åš shift
    price_series_close = df['Close'].loc[test_df_target.index[0] : test_df_target.index[-1]]
    # ä¸ºäº†å¾—åˆ°ç¬¬ä¸€å¤©çš„åŸºå‡†(æ˜¨æ—¥)ï¼Œæˆ‘ä»¬éœ€è¦å‰ä¸€å¤©çš„æ•°æ®
    prev_date = df.index[df.index.get_loc(test_df_target.index[0]) - 1]
    
    # æ„é€ "æ˜¨æ—¥ä»·æ ¼"åºåˆ—
    # å–å‡ºä» (æµ‹è¯•é›†ç¬¬ä¸€å¤©å‰ä¸€å¤©) åˆ° (æµ‹è¯•é›†å€’æ•°ç¬¬äºŒå¤©) çš„ä»·æ ¼
    ref_prices_high = df['High'].loc[prev_date : test_df_target.index[-2]].values
    ref_prices_close = df['Close'].loc[prev_date : test_df_target.index[-2]].values
    
    # ç¡®ä¿é•¿åº¦ä¸€è‡´
    assert len(ref_prices_close) == len(real_close), "åŸºå‡†ä»·æ ¼åºåˆ—é•¿åº¦ä¸åŒ¹é…"

    final_results_summary = []

    print(f"\n======== å¼€å§‹å®éªŒ (é¢„æµ‹ç›®æ ‡: æ”¶ç›Šç‡ -> è¿˜åŸä¸ºä»·æ ¼) ========")

    for exp_name, layers_config in EXPERIMENTS.items():
        print(f"\n>> [å®éªŒç»„]: {exp_name} ç»“æ„: {layers_config}")
        
        temp_maes = []
        temp_rmses = []
        temp_pred_high_list = []
        temp_pred_close_list = []

        for i in range(N_ROUNDS):
            print(f"   - ç¬¬ {i+1}/{N_ROUNDS} æ¬¡è®­ç»ƒ...", end="", flush=True)
            
            current_seed = BASE_SEED + i
            np.random.seed(current_seed)
            tf.random.set_seed(current_seed)
            tf.keras.backend.clear_session()
            
            model = build_generic_lstm_model(layers_config, (X_train.shape[1], X_train.shape[2]))
            model.fit(
                X_train, y_train, 
                epochs=EPOCHS, 
                batch_size=BATCH_SIZE, 
                verbose=0, 
                callbacks=[lr_schedule] 
            )
            
            # 1. é¢„æµ‹ (å¾—åˆ°å½’ä¸€åŒ–çš„æ”¶ç›Šç‡)
            pred_rets_scaled = model.predict(X_test, verbose=0)
            
            # 2. è¿˜åŸ (å½’ä¸€åŒ–æ”¶ç›Šç‡ -> çœŸå®æ”¶ç›Šç‡ -> çœŸå®ä»·æ ¼)
            rec_high, rec_close = recover_prices(
                pred_rets_scaled, scaler, 
                ref_prices_high, ref_prices_close, 
                TOTAL_FEATURES
            )
            
            # 3. è¯„ä¼° (å¯¹æ¯”è¿˜åŸåçš„ä»·æ ¼ vs çœŸå®ä»·æ ¼)
            rmse, mae = evaluate_predictions(real_close, rec_close)
            
            temp_maes.append(mae)
            temp_rmses.append(rmse)
            temp_pred_high_list.append(rec_high)
            temp_pred_close_list.append(rec_close)
            
            print(f" å®Œæˆ. (MAE: {mae:.2f})")

        # è®¡ç®—å¹³å‡
        avg_mae = np.mean(temp_maes)
        avg_rmse = np.mean(temp_rmses)
        avg_pred_high = np.mean(np.array(temp_pred_high_list), axis=0)
        avg_pred_close = np.mean(np.array(temp_pred_close_list), axis=0)
        
        print(f"   >> {exp_name} å¹³å‡ MAE: {avg_mae:.4f}")
        
        final_results_summary.append({
            "Experiment": exp_name,
            "Structure": str(layers_config),
            "Avg_MAE": avg_mae,
            "Avg_RMSE": avg_rmse,
            "Pred_High": avg_pred_high,
            "Pred_Close": avg_pred_close
        })

    results_df = pd.DataFrame(final_results_summary).sort_values(by="Avg_MAE")
    
    print("\n" + "="*60)
    print(f"æœ€ç»ˆå®éªŒæŠ¥å‘Š (æŒ‰ {N_ROUNDS} è½®å¹³å‡ MAE æ’åº)")
    print("="*60)
    print(results_df[["Experiment", "Structure", "Avg_MAE", "Avg_RMSE"]].to_string(index=False))
    
    best_exp = results_df.iloc[0]
    best_name = best_exp["Experiment"]
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹æ–¹æ¡ˆ: {best_name} (å¹³å‡MAE: {best_exp['Avg_MAE']:.4f})")

    # ç»˜å›¾
    print(f"\næ­£åœ¨ç»˜åˆ¶æœ€ä½³æ¨¡å‹ ({best_name}) çš„ç»“æœ...")
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)

    ax1.set_title(f'ä¸Šè¯ 2025 æœ€é«˜ä»·é¢„æµ‹ (åŸºäºæ”¶ç›Šç‡é¢„æµ‹è¿˜åŸ, {best_name})', fontsize=14, fontproperties=FONT_PROP)
    ax1.plot(dates, real_high, label='å®é™…æœ€é«˜ä»·', color='#d62728', linewidth=2)
    ax1.plot(dates, best_exp["Pred_High"], label='é¢„æµ‹æœ€é«˜ä»·(è¿˜åŸå)', color='#1f77b4', linestyle='--', linewidth=1.5)
    ax1.legend(loc='upper left', prop=FONT_PROP)
    ax1.grid(True, alpha=0.3)
    ax1.set_ylabel('ä»·æ ¼', fontproperties=FONT_PROP)

    ax2.set_title(f'ä¸Šè¯ 2025 æ”¶ç›˜ä»·é¢„æµ‹ (åŸºäºæ”¶ç›Šç‡é¢„æµ‹è¿˜åŸ, {best_name})', fontsize=14, fontproperties=FONT_PROP)
    ax2.plot(dates, real_close, label='å®é™…æ”¶ç›˜ä»·', color='#2ca02c', linewidth=2)
    ax2.plot(dates, best_exp["Pred_Close"], label='é¢„æµ‹æ”¶ç›˜ä»·(è¿˜åŸå)', color='#ff7f0e', linestyle='--', linewidth=1.5)
    ax2.legend(loc='upper left', prop=FONT_PROP)
    ax2.grid(True, alpha=0.3)
    ax2.set_ylabel('ä»·æ ¼', fontproperties=FONT_PROP)
    ax2.set_xlabel('æ—¥æœŸ', fontproperties=FONT_PROP)

    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
    plt.gcf().autofmt_xdate()
    plt.tight_layout()
    
    SAVE_NAME = f'LSTM_ReturnBased_Result_{best_name}.png'
    plt.savefig(SAVE_NAME, dpi=300)
    print(f"âœ… å›¾è¡¨å·²ä¿å­˜è‡³: {SAVE_NAME}")

if __name__ == "__main__":
    # å®šä¹‰ test_df_raw å˜é‡ä»¥ä¿®å¤å¼•ç”¨èŒƒå›´é—®é¢˜ (helper fix)
    test_df_raw = [] 
    main()